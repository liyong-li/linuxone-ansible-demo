---
#icic
local_rhel_repo: http://172.83.4.227:8080/local_repo/
http_rhel7_repo: http://172.83.4.227:8080/rhel79
cic_zvmguestconfig_path: "/home/data/offline/cic"

icic_vsw_name: VSW1
icic_instance_name: ins%05x
icic_zvm_profile_name: LNXDFLT
icic_admin_userid: IBMVM1
iso_server: 172.83.4.227
icicmgm_ip: 172.83.4.190

#storage and zone
stor_user: superuser
stor_host: 172.83.4.220
zone_user: lly
zone_host: 172.83.4.221
#stor_user: superuser
#stor_host: 172.83.4.220
#zone_user: admin
#zone_host: 172.83.4.221
#zhmc
# vault.yml file for the sample playbooks of the IBM Z HMC Collection

#San switch
dual_switch: false

zvm_compute_node: 172.83.4.227
out_path: /tmp




# hmc_auth parameter for the zhmc tasks, for the HMCs in the inventory:
ftp_ip: 172.83.4.227
linux_ftp_path: "iso/rhel84/generic.ins"
zvm_ftp_path: "iso/zvm72/CPDVD/720VM.INS"
ftp_username: "demoftp"
ftp_password: "demoftp"
ftp_install_path: "/home/demoftp/iso/install"
linux_inst_repo: "ftp://demoftp:demoftp@172.83.4.227/iso/rhel84"
linux_ks_path: "ftp://demoftp:demoftp@172.83.4.227/iso/install/ks"


zhmc_scripts: "{{ out_path }}/storage_requests"
zhmc_output: "{{ out_path }}/storage_requests"

hmc_ip: 172.83.4.223
hmchost: myhmc01
hmc_auth:
  userid: hmcapiuser
  password: Passw9rd
  verify: False
#hmc_auth:
#  myhmc01:  # must match the key name in the inventory file
#    userid: sysprog
#    password: passw0rd
#    verify: False

storsys_ip: "172.83.4.220" 
fabric_ip: "172.83.4.221"
storsans:
  id: "test"
  name: "test"          
  storsys_ip: "172.83.4.220" 
  storsys_username: "superuser" 
  fabric_ip: "172.83.4.221"
  fabric_username: "lly"  
  zone_prefix: ""
  zone_suffix: "_IBMSG_V7K1_N1N2CXPX"  
  zone_config: "IBMSG_SW1_CFG"  
  wwpn: ["IBMSG_V7K1_N1C1P1","IBMSG_V7K1_N1C1P2","IBMSG_V7K1_N2C1P1","IBMSG_V7K1_N2C1P2"]
  #{wwpn:"50:05:07:68:0b:25:32:04",alias_name:"IBMSG_V7K1_N1C1P1"},{wwpn:"50:05:07:68:0b:26:32:04",alias_name:"IBMSG_V7K1_N1C1P2"},{wwpn:"50:05:07:68:0b:25:32:05",alias_name:"IBMSG_V7K1_N2C1P1"},{wwpn:"50:05:07:68:0b:26:32:05",alias_name:"IBMSG_V7K1_N2C1P2"}




# The CPC the sample playbooks work with:
cpc_name: P005A078 
#stogroup_name: ["PROD_Linux001_SG01"]
stogroup_name: ["zvm4_os"]


#lpar_lists: ["zvm4"]

lpar_lists: ["DPP_DB_Ubuntu","DEMOZVM3","DEMOZVM4","DEMOZVM5","DEMOZVM1","DPP_HPVS"]

partition_name: TEST_CLONE
#DPP_HPVS
part_name: TEST_CLONE
adapter_name: "OSD 012C A01B-15"
stovolume_name: "10.00 GiB Data 208" 
user_name: "hmcapiuser"

part_type: linux
part_ifl_num: 4
part_min_memory: 16000
part_max_memory: 128000
part_min_weight: 50
part_max_weight: 800
part_init_weight: 100
nic_name: nic_1000
nic_adapter_name: "OSD 012C A01B-15"
nic_port: 0
nic_desc: "data network"
nic_addr: 1000
storage_mdisk_grp: 2
sgs: [
  {
  stogroup_name: "auto_sg_linux112_01",
  shared: false,
  path: 1,
  maxpart: 1,
  direct_connection_count: 0,
  vol_list: [{ vol_type: "boot", vol_num: 1, vol_size: "10", volname_prefix: "boot" }
             #{ vol_type: "data", vol_num: 1, vol_size: "50", volname_prefix: "data" },
             #{ vol_type: "data", vol_num: 2, vol_size: "100", volname_prefix: "data" }
            ]
  }
#  {
#  stogroup_name: "SG_TEST02",
#  shared: false,
#  path: 2,
#  maxpart: 1,
#  vol_list: [{ vol_type: "boot", vol_num: 1, vol_size: "49", volname_prefix: "boot" },
#             { vol_type: "data", vol_num: 3, vol_size: "99", volname_prefix: "data" }
#            ]
#  }
]

#dpp
#dpp_cli_dir: "/root/HPVS124/HPDC-1.2.2-GA"
#dpp_ui_dir: "/root/HPVS124/HPDC-1.2.2-GA-Management-Interface"
#dpp_ssc_uid: "admin"
#dpp_ssc_pwd: "adminadmin"
#dpp_ip:      "172.83.4.222"
# cca
install_binary_cca: "/home/data/offline/pe"
pvol: "v21"
pvoldev: "/dev/mapper/mpatha"
apqns: "02.0002"
#apqns: "01.0006"

# ocp identity provider
ocp_idp: "OCP_HTPASSWD"
ocp_idp_users_secret: "users-secret" 
workdir: /root/ocp4-workdir
nfs_server: 172.83.4.63
controller_schedulable: false

ocp_operators: ["local-storage-operator",
                 "ocs-operator",
                 "kiali",
                 "servicemeshoperator",
                 "jaeger-product",
                 "codeready-workspaces"
]

#proxy
#install_proxy: "192.168.196.22"
#z/VM FTP
zvm_ftp: "172.83.4.208"
zvm_ftp_user: "LNXMAINT"
zvm_ftp_pwd: "dfltpass" 
# configure Yum
source_iso: "/home/demoftp/RHEL-8.4.0-20210503.1-s390x-dvd1.iso"
dest_iso: "/mnt/RHEL-8.4.0-20210503.1-s390x-dvd1.iso"
repo_mount: "/mnt/rhel"
#docker
docker_compose_version: "1.29.1"
# Configure the internal domain name of the cluster here.
cluster_base_domain: "test123.com"
cluster_domain_name: "{{ cluster_name }}.{{ cluster_base_domain }}"
cluster_name: cloud

# The URL of of the archive containing the
# RHCOS kernel, initramfs, and disk images
rhcos_version: "4.10.3-s390x"
ocp_version: "4.10"
ocp_minor_version: "4.10.3"

# The public DNS server to use.  This should be
# changed to a DNS server on your intranet.
dns_nameserver: 8.8.8.8

# The IP address of the local network
# gateway, and the netmask/cidr of
# the subnet
subnet_gateway: 172.83.4.97
subnet_netmask: 255.255.255.0
subnet_cidr: 172.83.4.0/24
subnet_in_addr_name: "{{ subnet_cidr.split('.')[:3] | reverse | join('.') }}"

# This is the IP address of the bastion which the
# cluster nodes can see.
bastion_private_ip_address: 172.83.4.63
# This is the IP address of the bastion
# when the bastion is configures as a dhcp
# server.  Otherwise it is ignored.
dhcp_server_ip_address: 192.168.196.2

# z/VM kernel parameters
# User must supply zvm rd.znet device id
# based on their own environment.
# example:
# "rd.znet=qeth,0.0.1000,0.0.1001,0.0.1002,layer2=1,portno=0"
zvm_rd_znet: "rd.znet=qeth,0.0.1000,0.0.1001,0.0.1002,layer2=1,portno=0"
# User must supply zvm rd.dasd device id kernel arg.
# This is the device to install coreos to.
# WARNING - using the wrong device id could result in loss of data.  The
# device used in this example is not necessarily correct for your
# environment.
# example:
# "rd.dasd=0.0.0120"
zvm_rd_dasd: ""
zvm_edev_vdev: 2000

kvm_macvtap_nic_name: enc1000
kvm_image_dir: /var/lib/libvirt/images/nova/instances

coreos_kargs_net:
  zvm: "ip={{ cluster_nodes[coreos_role][item].ip }}::{{ subnet_gateway }}:{{ subnet_netmask }}:::none nameserver={{ bastion_private_ip_address }}"
  zkvm: "ip={{ cluster_nodes[coreos_role][item].ip }}::{{ subnet_gateway }}:{{ subnet_netmask }}:::none nameserver={{ bastion_private_ip_address }}"


dev_disk:
  zvm: /dev/mapper/mpatha 
  zkvm: vda

zvm_host: 172.83.4.208
smapi_user: IBMVM1
smapi_password: admin123

linux_nodes:
  zvm:
    icicmgmt:
      ip: '172.83.4.204'
      gw: '172.83.4.97'
      mask: 24
      zvm_uname: 'ICICMGMT'
      vcpu: 4
      init_mem: 8
      max_mem: 16
      sg_groups_boot: "SG_ICIC_MGM"
      sg_groups_data: []
  kvm:
    null
    
# This is the default node configuration.
# Please ensure the MAC addresses are
# correct for the hardware you are deploying to.
# The IP addresses must be on the same subnet as
# the {{ cluster_subnet_prefix }}
# CHANGEME!
cluster_nodes:
  bootstrap:
    ocpbootstrap-0:
      mac: '52:54:00:d0:f7:03'
      ip: '172.83.4.70'
      disk: "{{ dev_disk[install_mode] }}"
      ign_profile: bootstrap.ign
      zvm_uname: 'R8400024'
      zfcp_disk:
        - "rd.zfcp=0.0.002f,0x500507680b263204,0x0000000000000000"
        - "rd.zfcp=0.0.002b,0x500507680b253205,0x0000000000000000"
  masters:
    ocpmaster-0:
      mac: '52:54:00:da:70:41'
      ip: '172.83.4.71'
      disk: "{{ dev_disk[install_mode] }}"
      ign_profile: master.ign
      zvm_uname: 'R8400025'
      zfcp_disk:
        - "rd.zfcp=0.0.0021,0x500507680b263205,0x0000000000000000"
        - "rd.zfcp=0.0.0017,0x500507680b253204,0x0000000000000000"
    ocpmaster-1:
      mac: '52:54:00:a2:93:d8'
      ip: '172.83.4.72'
      disk: "{{ dev_disk[install_mode] }}"
      ign_profile: master.ign
      zvm_uname: 'R8400026'
      zfcp_disk:
        - "rd.zfcp=0.0.0019,0x500507680b253205,0x0000000000000000"
        - "rd.zfcp=0.0.0031,0x500507680b263204,0x0000000000000000"
    ocpmaster-2:
      mac: '52:54:00:d9:5b:d8'
      ip: '172.83.4.73'
      disk: "{{ dev_disk[install_mode] }}"
      ign_profile: master.ign
      zvm_uname: 'R840002C'
      zfcp_disk:
        - "rd.zfcp=0.0.001b,0x500507680b253205,0x0000000000000000"
        - "rd.zfcp=0.0.001e,0x500507680b263204,0x0000000000000000"
  workers:
    ocpworker-0:
      mac: '52:54:00:70:8b:74'
      ip: '172.83.4.74'
      disk: "{{ dev_disk[install_mode] }}"
      ign_profile: worker.ign
      zvm_uname: 'R840002D'
      zfcp_disk:
        - "rd.zfcp=0.0.001b,0x500507680b263204,0x0000000000000000"
        - "rd.zfcp=0.0.002c,0x500507680b253205,0x0000000000000000"
    ocpworker-1:
      mac: '52:54:00:70:8b:75'
      ip: '172.83.4.75'
      disk: "{{ dev_disk[install_mode] }}"
      ign_profile: worker.ign
      zvm_uname: 'R840002E'
      zfcp_disk:
        - "rd.zfcp=0.0.0016,0x500507680b253204,0x0000000000000000"
        - "rd.zfcp=0.0.0027,0x500507680b263205,0x0000000000000000"
    ocpworker-2:
      mac: '52:54:00:70:8b:76'
      ip: '172.83.4.76'
      disk: "{{ dev_disk[install_mode] }}"
      ign_profile: worker.ign
      zvm_uname: 'R840002F'
      zfcp_disk:
        - "rd.zfcp=0.0.0024,0x500507680b263204,0x0000000000000000"
        - "rd.zfcp=0.0.001d,0x500507680b253205,0x0000000000000000"

# The following variables should generally not need to be edited

install_mode: zvm
#install_mode: zkvm

ansible_client_id: lly

controller_node: ocpmaster-0.cloud.test123.com

# This will be the same as the bastion private ip when
# bastion is a separate VM, otherwise it is the gateway ip.
# dhcp_server_ip_address is not used by default in a z/VM install
bastion_public_ip_address: "{{ ansible_default_ipv4.address }}"

# pxe_mode (grub, pxelinux)
# use pxelinux mode for zvm to generate
# parmfiles, then punch and IPL manually
pxe_mode: pxelinux

#ocp4_pull_secret: "{{ lookup('file', 'ocp4_pull_secret') | from_json }}"

rhcos_disk_type:
  zvm: metal-dasd
  zkvm: scsi
rhcos_download_url: "https://mirror.openshift.com/pub/openshift-v4/s390x/dependencies/rhcos/{{ ocp_version }}/{{ ocp_minor_version }}"
#offline_mode: "offline"
rhcos_download_url_offline: "/home/data/offline/ocp"

# The download URL of the openshift-install binary
openshift_install_binary_url: "https://mirror.openshift.com/pub/openshift-v4/s390x/clients/ocp/{{ ocp_minor_version }}" 
openshift_install_binary_url_offline: "/home/data/offline/ocp"
install_binary_url_offline: "/home/data/offline"
openshift_install_cli: 'openshift-install-linux-{{ ocp_minor_version }}.tar.gz'
openshift_client_cli: 'openshift-client-linux-{{ ocp_minor_version }}.tar.gz'
rhcos_installer_kernel: 'rhcos-{{ rhcos_version }}-live-kernel-s390x'
rhcos_installer_initramfs: 'rhcos-{{ rhcos_version }}-live-initramfs.s390x.img'
rhcos_disk_image: 'rhcos-{{ rhcos_version }}-live-rootfs.s390x.img'
rhcos_disk_image_short: 'rootfs.img'

rhcos_download_files:
  - '{{ rhcos_installer_initramfs }}'
  - '{{ rhcos_installer_kernel }}'
rhcos_rootfs_files:
  - '{{ rhcos_disk_image }}'

coreos_kargs_extra:
  zvm:  "{{ zvm_rd_znet }}"
  zkvm: "rd.neednet=1 dfltcc=off coreos.inst=yes coreos.inst.install_dev=vda coreos.live.rootfs_url=http://{{bastion_private_ip_address}}:8080/rootfs/{{ rhcos_disk_image_short }} {{ coreos_kargs_net[install_mode] }}  coreos.inst.ignition_url=http://{{bastion_private_ip_address}}:8080/ignition/{{ cluster_nodes[coreos_role][item].ign_profile }}"

